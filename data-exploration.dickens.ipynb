{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EB5002 Text Processing\n",
    "\n",
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tkokkeng/Documents/EB5002-TextProcessing'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(os.path.join(os.path.sep, 'home', 'tkokkeng', 'Documents', 'EB5002-TextProcessing'))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/tkokkeng/Documents/EB5002-TextProcessing',\n '/home/tkokkeng/Documents/EB5002-TextProcessing/source',\n '/home/tkokkeng/python/python367/ptvenv/lib/python36.zip',\n '/home/tkokkeng/python/python367/ptvenv/lib/python3.6',\n '/home/tkokkeng/python/python367/ptvenv/lib/python3.6/lib-dynload',\n '/usr/lib/python3.6',\n '',\n '/home/tkokkeng/python/python367/ptvenv/lib/python3.6/site-packages',\n '/home/tkokkeng/.local/lib/python3.6/site-packages',\n '/usr/local/lib/python3.6/dist-packages',\n '/usr/lib/python3/dist-packages',\n '/home/tkokkeng/.local/lib/python3.6/site-packages/IPython/extensions',\n '/home/tkokkeng/.ipython']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "if os.path.join(os.getcwd(), 'source') not in sys.path:\n",
    "    sys.path.append(os.path.join(os.getcwd(), 'source'))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, pre-processed and save to file.\n",
    "\n",
    "Before, the top and bottom non-relevant paragraphs were removed manually from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(os.path.join('data', '1289-0-preprocessed.txt'), mode='w', encoding='utf-8')\n",
    "if os.path.isfile(os.path.join('data', '1289-0-lesstopbottom.txt')):\n",
    "    with open(os.path.join('data', '1289-0-lesstopbottom.txt'), mode='r', encoding='utf-8-sig') as infile:  # encoding to ignore Byte Order Marker (BOM)\n",
    "        for a_line in infile:\n",
    "            if not (re.match('^\\ *\\[.*\\]$', a_line)  # remove e.g. [Picture: ...]\n",
    "                    or a_line.isupper()  # remove titles\n",
    "                    or re.match('^\\ *Chapter|\\ *chapter|\\ *CHAPTER', a_line)  # remove e.g. Chapter I\n",
    "                    or re.match('^\\ *[\\*]+\\ *', a_line)):  # remove combinations of * * * * * * *\n",
    "                outfile.write(a_line)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', '1289-0-preprocessed.txt'), mode='r', encoding='utf-8') as infile:\n",
    "    book1 = infile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNDER none of the accredited ghostly circumstances, and environed by none\\nof the conventional ghostly surroundings, did I first make acquaintance\\nwith the house which is the subject of this Christmas piece.  I saw it in\\nthe daylight, with the sun upon it.  There was no wind, no rain, no\\nlightning, no thunder, no awful or unwonted circumstance, of any kind, to\\nheighten its effect.  More than that: I had come to it direct from a\\nrailway station: it was not more than a mile distant from the railway\\nstation; and, as I stood outside the house, looking back upon the way I\\nhad come, I could see the goods train running smoothly along the\\nembankment in the valley.  I will not say that everything was utterly\\ncommonplace, because I doubt if anything can be that, except to utterly\\ncommonplace people—and there my vanity steps in; but, I will take it on\\nmyself to say that anybody might see the house as I saw it, any fine\\nautumn morning.\\n\\nThe manner of my lighting on it was this.\\n\\nI was travelling towards London out of the North, intending to stop by\\nthe way, to look at the house.  My health required a temporary residence\\nin the country; and a friend of mine who knew that, and who had happened\\nto drive past the house, had written to me to suggest it as a likely\\nplace.  I had got into the train at midnight, and had fallen asleep, and\\nhad woke up and had sat looking out of window at the brilliant Northern\\nLights in the sky, and had fallen asleep again, and had woke up again to\\nfind the night gone, with the usual discontented conviction on me that I\\nhadn’t been to sleep at all;—upon which question, in the first imbecility\\nof that condition, I am ashamed to believe that I would have done wager\\nby battle with the man who sat opposite me.  That opposite man had had,\\nthrough the night—as that opposite man always has—several legs too many,\\nand all of them too long.  In addition to this unreasonable conduct\\n(which was only to be expected of him), he had had a pencil and a\\npocket-book, and '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book1[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    # Testing whether it works. \n",
    "    # Sometimes it doesn't work on some machines because of setup issues.\n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    # See https://stackoverflow.com/a/25736515/610569\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    # Use the toktok tokenizer that requires no dependencies.\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = toktok.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1_tokenised = [ list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(book1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['under',\n 'none',\n 'of',\n 'the',\n 'accredited',\n 'ghostly',\n 'circumstances',\n ',',\n 'and',\n 'environed',\n 'by',\n 'none',\n 'of',\n 'the',\n 'conventional',\n 'ghostly',\n 'surroundings',\n ',',\n 'did',\n 'i',\n 'first',\n 'make',\n 'acquaintance',\n 'with',\n 'the',\n 'house',\n 'which',\n 'is',\n 'the',\n 'subject',\n 'of',\n 'this',\n 'christmas',\n 'piece',\n '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book1_tokenised[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book1_tokenised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed, tokenised and save data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['1289-0', '1400-0', '1467-0', '27924-0', '564-0']  # add all the filenames of the books here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removed the top and bottom irrelevant parts of the text manually and save the files to the folder .../manual-processed\n",
    "* The pre-processed files will be saved to the folder .../pre-processed\n",
    "* The consolidated tokenised data will be saved in .../final/all_books.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books = []\n",
    "\n",
    "for filename in filenames:\n",
    "    \n",
    "    outfile = open(os.path.join('data', 'pre-processed', filename + '-preprocessed.txt'), mode='w', encoding='utf-8')\n",
    "    if os.path.isfile(os.path.join('data', 'manual-processed', filename + '-lesstopbottom.txt')):\n",
    "        with open(os.path.join('data', 'manual-processed', filename + '-lesstopbottom.txt'), mode='r', encoding='utf-8-sig') as infile:  # encoding to ignore Byte Order Marker (BOM)\n",
    "            for a_line in infile:\n",
    "                if not (re.match('^\\ *\\[.*\\]$', a_line)  # remove e.g. [Picture: ...]\n",
    "                        or a_line.isupper()  # remove titles\n",
    "                        or re.match('^\\ *Chapter|\\ *chapter|\\ *CHAPTER', a_line)  # remove e.g. Chapter I\n",
    "                        or re.match('^\\ *[\\*]+\\ *', a_line)):  # remove combinations of * * * * * * *\n",
    "                    outfile.write(a_line)\n",
    "    outfile.close()\n",
    "    \n",
    "    with open(os.path.join('data', 'pre-processed', filename + '-preprocessed.txt'), mode='r', encoding='utf-8') as infile:\n",
    "        a_book = infile.read()\n",
    "        \n",
    "    a_book_tokenised = [ list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(a_book) ]\n",
    "    all_books += (a_book_tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open(os.path.join('data', 'final', 'all_books.txt'), 'w', encoding='utf8') as outfile:\n",
    "    json.dump(all_books, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open(os.path.join('data', 'final', 'all_books.txt'), encoding='utf8') as infile:\n",
    "    tokenized_text = json.load(infile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8975"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n 'informed',\n 'him',\n 'in',\n 'exchange',\n 'that',\n 'my',\n 'christian',\n 'name',\n 'was',\n 'philip.',\n '“i',\n 'don',\n '’',\n 't',\n 'take',\n 'to',\n 'philip',\n ',',\n '”',\n 'said',\n 'he',\n ',',\n 'smiling',\n ',',\n '“for',\n 'it',\n 'sounds',\n 'like',\n 'a',\n 'moral',\n 'boy',\n 'out',\n 'of',\n 'the',\n 'spelling-book',\n ',',\n 'who',\n 'was',\n 'so',\n 'lazy',\n 'that',\n 'he',\n 'fell',\n 'into',\n 'a',\n 'pond',\n ',',\n 'or',\n 'so',\n 'fat',\n 'that',\n 'he',\n 'couldn',\n '’',\n 't',\n 'see',\n 'out',\n 'of',\n 'his',\n 'eyes',\n ',',\n 'or',\n 'so',\n 'avaricious',\n 'that',\n 'he',\n 'locked',\n 'up',\n 'his',\n 'cake',\n 'till',\n 'the',\n 'mice',\n 'ate',\n 'it',\n ',',\n 'or',\n 'so',\n 'determined',\n 'to',\n 'go',\n 'a',\n 'bird',\n '’',\n 's-nesting',\n 'that',\n 'he',\n 'got',\n 'himself',\n 'eaten',\n 'by',\n 'bears',\n 'who',\n 'lived',\n 'handy',\n 'in',\n 'the',\n 'neighborhood',\n '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_books[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
